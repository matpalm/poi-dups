near duplicate detection with ngram weighting

1. sql to dump pois (pois.tsv)
sql> select p.id as poi_id, pl.name as poi_name, places.id as place_id
sql>  from pois p join places on p.place_id=places.id 
sql>  join poi_localisations pl on pl.poi_id=p.id
(or hit public api)

2. sql to dump places (places.tsv)
sql> select id,name,full_name from places
(or hit public api)

3. split into 4 files

$ ./split_based_on_places.rb < pois.tsv

4. run near duplicate detection

$ cat pois.p0.tsv | ./near_dups.rb > near_dups.p0.out &
$ cat pois.p1.tsv | ./near_dups.rb > near_dups.p1.out &
$ cat pois.p2.tsv | ./near_dups.rb > near_dups.p2.out &
$ cat pois.p3.tsv | ./near_dups.rb > near_dups.p3.out &
$ wait

5. run connected components analysis

$ cat near_dups.p*.out | ./connected_components.rb > connected_components.out
1.000     420197  420147
1.000     1112845 1112889 1072699 1072704 1112865 1112878
0.869     1037033 1037044 1037047
0.826     1076554 381583  381207
0.619     401746  401702
1.000     399763  1041146

6. make into csv

$ ./reportify.rb

# WIP

time cat pois.tsv | cut -f2 | ./tokens.rb > token_to_norm_freq.tsv
2sec
33,000 lines







