near duplicate detection 
(using token frequency weighting to dictate ngram contribution to similarity)

1. sql to dump pois (pois.tsv)
sql> select p.id as poi_id, pl.name as poi_name, places.id as place_id
sql>  from pois p join places on p.place_id=places.id 
sql>  join poi_localisations pl on pl.poi_id=p.id
(or hit public api)

2. sql to dump places (places.tsv)
sql> select id,name,full_name from places
(or hit public api)

2.5 cleanup last run

$ rm pois.p*tsv near_dups_v2.p*out

3. split into 4 files

$ ./split_based_on_places.rb < pois.tsv

------------------------
-- VERSION 1
- each ngram is weighted the same

4. run near duplicate detection

cat pois.p0.tsv | ./near_dups.rb > near_dups.p0.out &
cat pois.p1.tsv | ./near_dups.rb > near_dups.p1.out &
cat pois.p2.tsv | ./near_dups.rb > near_dups.p2.out &
cat pois.p3.tsv | ./near_dups.rb > near_dups.p3.out &
wait

5. run connected components analysis

$ cat near_dups.p*.out | ./connected_components.rb > connected_components.out
1.000     420197  420147
1.000     1112845 1112889 1072699 1072704 1112865 1112878
0.869     1037033 1037044 1037047
0.826     1076554 381583  381207
0.619     401746  401702
1.000     399763  1041146

6. make into csv

$ ./reportify.rb

------------------------
-- VERSION 2
- ngrams weighted based on the frequency of the tokens they come from.

4. generate tokens hash

$ cut -f2 pois.tsv | ./generate_tokens_freq_lookup.rb

5. process

cat pois.p0.tsv | ./near_dups_v2.rb > near_dups_v2.p0.out &
cat pois.p1.tsv | ./near_dups_v2.rb > near_dups_v2.p1.out &
cat pois.p2.tsv | ./near_dups_v2.rb > near_dups_v2.p2.out &
cat pois.p3.tsv | ./near_dups_v2.rb > near_dups_v2.p3.out &
wait

6. run connected components analysis

$ cat near_dups_v2.p*.out | ./connected_components.rb > connected_components.out
1.000     420197  420147
1.000     1112845 1112889 1072699 1072704 1112865 1112878
0.869     1037033 1037044 1037047
0.826     1076554 381583  381207
0.619     401746  401702
1.000     399763  1041146

7. make into csv

$ ./reportify.rb





